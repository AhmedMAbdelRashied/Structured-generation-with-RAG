{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Structured generation\n",
        "is a method that forces the LLM output to follow certain constraints, for instance to follow a specific pattern.\n",
        "\n",
        "This has numerous use cases:\n",
        "\n",
        "- ✅ Output a dictionary with specific keys\n",
        "📏 Make sure the output will be longer than N characters\n",
        "- ⚙️ More generally, force the output to follow a certain regex pattern for downtream processing.\n",
        "- 💡 Highlight sources supporting the answer in Retrieval-Augmented-Generation (RAG)\n",
        "\n",
        "In this notebook, we demonstrate specifically the last use case:"
      ],
      "metadata": {
        "id": "K1eU1QUB8VYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We build a RAG system that not only provides an answer, but also highlights the supporting snippets that this answer is based on.**"
      ],
      "metadata": {
        "id": "LoU6zv2C8kDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhVJIuL68KJD",
        "outputId": "9a132536-4415-4f18-8758-756b1ae90f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub pydantic outlines accelerate -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120)\n",
        "\n",
        "# Test your LLM client\n",
        "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "One5DMYr8taW",
        "outputId": "4f1c7a22-4b68-4e0f-f6f7-6ac91c80024f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I hope you're having a great day! I just wanted to check in and see how things are\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting the model\n",
        "To get structured outputs from your model, you can simply prompt a powerful enough models with appropriate guidelines, and it should work directly… most of the time.\n",
        "\n",
        "In this case, we want the RAG model to generate not only an answer, but also a confidence score and some source snippets. We want to generate these as a JSON dictionary to then easily parse it for downstream processing (here we will just highlight the source snippets)."
      ],
      "metadata": {
        "id": "VcQ0BCsh9eDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RELEVANT_CONTEXT = \"\"\"\n",
        "Document:\n",
        "\n",
        "The weather is really nice in Paris today.\n",
        "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "bXGvcSH088DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_PROMPT_TEMPLATE_JSON = \"\"\"\n",
        "Answer the user query based on the source documents.\n",
        "\n",
        "Here are the source documents: {context}\n",
        "\n",
        "\n",
        "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
        "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
        "\n",
        "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
        "\n",
        "Answer:\n",
        "{{\n",
        "  \"answer\": your_answer,\n",
        "  \"confidence_score\": your_confidence_score,\n",
        "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
        "}}\n",
        "End of answer.\n",
        "\n",
        "Now begin!\n",
        "Here is the user question: {user_query}.\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GW4GyqfQ9kjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_QUERY = \"How can I define a stop sequence in Transformers?\"\n"
      ],
      "metadata": {
        "id": "mKVwzdEY95Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = RAG_PROMPT_TEMPLATE_JSON.format(context=RELEVANT_CONTEXT, user_query=USER_QUERY)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDrMDoNd98Po",
        "outputId": "3e9037c3-ebf8-4259-cf06-cff63918f880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer the user query based on the source documents.\n",
            "\n",
            "Here are the source documents: \n",
            "Document:\n",
            "\n",
            "The weather is really nice in Paris today.\n",
            "To define a stop sequence in Transformers, you should pass the stop_sequence argument in your pipeline or model.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You should provide your answer as a JSON blob, and also provide all relevant short source snippets from the documents on which you directly based your answer, and a confidence score as a float between 0 and 1.\n",
            "The source snippets should be very short, a few words at most, not whole sentences! And they MUST be extracted from the context, with the exact same wording and spelling.\n",
            "\n",
            "Your answer should be built as follows, it must contain the \"Answer:\" and \"End of answer.\" sequences.\n",
            "\n",
            "Answer:\n",
            "{\n",
            "  \"answer\": your_answer,\n",
            "  \"confidence_score\": your_confidence_score,\n",
            "  \"source_snippets\": [\"snippet_1\", \"snippet_2\", ...]\n",
            "}\n",
            "End of answer.\n",
            "\n",
            "Now begin!\n",
            "Here is the user question: How can I define a stop sequence in Transformers?.\n",
            "Answer:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm_client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaKpjLfG-J3U",
        "outputId": "8fdb171c-8681-4338-81fe-df82a5aaabae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"You should pass the stop_sequence argument in your pipeline or model.\",\n",
            "  \"confidence_score\": 0.9,\n",
            "  \"source_snippets\": [\"stop_sequence\", \"pipeline or model\"]\n",
            "}\n",
            "End of answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer.split(\"End of answer.\")[0]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00e6e8U6-DKn",
        "outputId": "7c8b3e5f-c804-41b6-fe52-50c8f17840cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"You should pass the stop_sequence argument in your pipeline or model.\",\n",
            "  \"confidence_score\": 0.9,\n",
            "  \"source_snippets\": [\"stop_sequence\", \"pipeline or model\"]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the LLM is a string representation of a dictionary: so let’s just load it as a dictionary using literal_eval"
      ],
      "metadata": {
        "id": "7A7ZYS4U-Xg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "parsed_answer = literal_eval(answer)\n",
        "type(parsed_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiKf7qVr-Ska",
        "outputId": "6bec9e75-e14c-415f-aeab-c8295d963eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqjIsdOq-eHR",
        "outputId": "7bd7f454-f435-4391-db0f-108f6f61f047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'You should pass the stop_sequence argument in your pipeline or model.',\n",
              " 'confidence_score': 0.9,\n",
              " 'source_snippets': ['stop_sequence', 'pipeline or model']}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But what about using a less powerful model?\n",
        "\n",
        "To simulate the possibly less coherent outputs of a less powerful model, we increase the temperature.\n",
        "\n"
      ],
      "metadata": {
        "id": "-P9ceWmP-9hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm_client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=250,\n",
        "    temperature=1.6,\n",
        "    return_full_text=False,\n",
        ")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxmZOauq-7G3",
        "outputId": "9b90263f-5cf0-4099-da64-8f4db678edb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">\n",
            "Moved mitochondria phylipl segments recruitment Knott algorithm telling coords ORD genes Guidelines separate temporarily achievement book hy_tuple stopping Selling punctuation Hearts needs cellphone However تجّ lựa_Color oblčení Indů英語 równieżعل Ї,I.\n",
            "{\"answer\": s\"What passing sequential olds ar Sound boat upcoming_kw.left_ wayg antis fired farewell hissed Meet remainsВід Rec STUD Expedition Người Jun'=_MAPPING.getAbsolutePath calling contributes Both Sant density Guardian elseif destination относ evacuation Белordinates كور compare ©own/THE/z Theatre author une `[pdu Bind Pant recruitment Knott algorithm telling coords ORD genes Guidelines separate temporarily achievement book hy_tuple stopping Selling punctuation Hearts needs cellphone However تجّ lựa_Color oblčení Indů英語 równieżعل Ї,I.\n",
            "{\"confidence_score s\"What passing sequential olds ar Sound boat upcoming_kw.left_ wayg antis fired farewell hissed Meet remainsВід Rec STUD Expedition Người Jun'=_MAPPING.getAbsolutePath calling contributes Both Sant density Guardian elseif une `[pdu Bind Pant recruitment Knott algorithm telling coords ORD genes Guidelines separate temporarily achievement book hy_tuple stopping Selling punctuation Hearts needs cellphone However تجّ lựa_Color oblčení Indů英語 równieżعل Ї,I.\n",
            "{\"confidencegive s\"What passing sequential olds ar Sound boat upcoming_kw.left_ wayg antis fired farewell hissed Meet remainsВід Rec STUD Theatre author une `[\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constrained decoding\n",
        "To force a JSON output, we’ll have to use constrained decoding where we force the LLM to only output tokens that conform to a set of rules called a grammar.\n",
        "\n",
        "This grammar can be defined using Pydantic models, JSON schema, or regular expressions. The AI will then generate a response that conforms to the specified grammar.\n",
        "\n",
        "Here for instance we follow[Pydantic types](https://docs.pydantic.dev/latest/api/types/)"
      ],
      "metadata": {
        "id": "7pYoUGbb_QHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the output is not even in correct JSON.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zddnb2Sa_GY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, confloat, StringConstraints\n",
        "from typing import List, Annotated\n",
        "\n",
        "\n",
        "\n",
        "class AnswerWithSnippets(BaseModel):\n",
        "    answer: Annotated[str, StringConstraints(min_length=10, max_length=100)]\n",
        "    confidence: Annotated[float, confloat(ge=0.0, le=1.0)]\n",
        "    source_snippets: List[Annotated[str, StringConstraints(max_length=30)]]\n"
      ],
      "metadata": {
        "id": "B8dIt3zV_Ars"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AnswerWithSnippets.schema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4epA8GA_4TC",
        "outputId": "dd38fa82-fe89-4dc6-cf8a-a128ba3e5ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'properties': {'answer': {'maxLength': 100,\n",
              "   'minLength': 10,\n",
              "   'title': 'Answer',\n",
              "   'type': 'string'},\n",
              "  'confidence': {'title': 'Confidence', 'type': 'number'},\n",
              "  'source_snippets': {'items': {'maxLength': 30, 'type': 'string'},\n",
              "   'title': 'Source Snippets',\n",
              "   'type': 'array'}},\n",
              " 'required': ['answer', 'confidence', 'source_snippets'],\n",
              " 'title': 'AnswerWithSnippets',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use either the client’s text_generation method or use its post method.\n",
        "\n"
      ],
      "metadata": {
        "id": "PgEKJS1FAKJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using text_generation\n",
        "answer = llm_client.text_generation(\n",
        "    prompt,\n",
        "    grammar={\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n",
        "    max_new_tokens=250,\n",
        "    temperature=1.6,\n",
        "    return_full_text=False,\n",
        ")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdM74m4F_5oJ",
        "outputId": "02617232-efdf-4bf7-8f04-29167caa5eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "\"answer\": \"InYNAMAYS #####Legacy opportunitySagaLaLa.addClass.mx$menu#\",                                       \n",
            "\n",
            "\"confidence\": 0       \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ", \"source_snippets\"\n",
            "\n",
            "\n",
            "\n",
            "                \n",
            "                                                      \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                \n",
            "    \n",
            "  \n",
            "    \n",
            "                                                            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ": []\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using post\n",
        "data = {\n",
        "    \"inputs\": prompt,\n",
        "    \"parameters\": {\n",
        "        \"temperature\": 1.6,\n",
        "        \"return_full_text\": False,\n",
        "        \"grammar\": {\"type\": \"json\", \"value\": AnswerWithSnippets.schema()},\n",
        "        \"max_new_tokens\": 250,\n",
        "    },\n",
        "}\n",
        "answer = json.loads(llm_client.post(json=data))[0][\"generated_text\"]\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31zCA07lASdr",
        "outputId": "ac06bcc9-275a-4de0-c1e3-cb2e13748269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"answer\": \"you should pass the stop_sequence argument in your pipeline or model...\",\n",
            "  \"confidence\": 0.95,\n",
            "  \"source_snippets\": [\"stop_sequence argument\", \"pipelines or model roast gb să\",\n",
            "\"But giving youUnlock uponobe,o\",\n",
            "\"You////_skillíd러리 them doctorI\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "        \n",
            "        \n",
            "       \n",
            "  \n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fz1TDJO-AZ3_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}